{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af96748f",
   "metadata": {},
   "source": [
    "## Chinese Character Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.path as mplpath\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb4b6f",
   "metadata": {},
   "source": [
    "## Part 1: data preparation\n",
    "\n",
    "First of all, we need to specify a couple of paths which point to the following files:\n",
    "\n",
    "  - An image directory\n",
    "  - A directory containing an `info.json` file which documents metadata about the images, such as height and width in pixels. This directory should also contain a `jsonl` file which specifies the locations of the bounding boxes in each image.\n",
    "  \n",
    "These variables are defined below, and need to be populated before the notebook can be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eb6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"ENTER PATH HERE\"        # For example: \"/scratch/lt2326-h21/a1/images\"\n",
    "JSON_DIR = \"ENTER JSON PATH HERE\"    # For example: \"/scratch/lt2326-h21/a1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61616eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = json.load(open(os.path.join(JSON_DIR, \"info.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216b124",
   "metadata": {},
   "source": [
    "Check that all images have the same height and width in pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32500466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(i[\"height\"] for i in info[\"train\"]))\n",
    "print(set(i[\"width\"] for i in info[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffda7a",
   "metadata": {},
   "source": [
    "Open train JSON file and check that the length is the same as `info.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ee901",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(JSON_DIR, \"train.jsonl\")) as file:\n",
    "    data = [json.loads(x) for x in file][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390ecc9",
   "metadata": {},
   "source": [
    "Filter down the entries in `data` to just those which are actually in `images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = os.listdir(IMAGE_DIR)\n",
    "test_data = [item for item in data if item[\"file_name\"] in image_list]\n",
    "print(f\"There are {len(test_data)} items in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d046cc",
   "metadata": {},
   "source": [
    "## Extract and Process Polygons for Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = np.array([[[a,b] for b in list(range(2048))] for a in list(range(2048))]).reshape((2048 * 2048, 2))\n",
    "\n",
    "def truncate(number, digits = 0) -> int:\n",
    "    stepper = 10.0 ** digits\n",
    "    return int(math.trunc(stepper * number) / stepper)\n",
    "\n",
    "\n",
    "def get_truncated_polygons(annotations):\n",
    "    polygons = []\n",
    "\n",
    "    for sentence in annotations:\n",
    "        for character in sentence:\n",
    "            if character[\"is_chinese\"]:\n",
    "                truncated = []\n",
    "                for (x, y) in character[\"polygon\"]:\n",
    "                    truncated.append([truncate(x), truncate(y)])\n",
    "                polygons.append(truncated)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def get_polygon_matrix(annotations):\n",
    "    polygons = get_truncated_polygons(annotations)\n",
    "        \n",
    "    truth_values = np.array([False] * GRID.shape[0])\n",
    "    list_of_paths = [mplpath.Path(arr) for arr in np.array(polygons)]\n",
    "    \n",
    "    for path in list_of_paths:\n",
    "        truth_values += path.contains_points(GRID)\n",
    "    \n",
    "    truth_values = np.asarray(truth_values, int).reshape((2048, 2048)).T\n",
    "    return truth_values\n",
    "    \n",
    "def convert_image_to_numpy_array(file_name):\n",
    "    path = os.path.join(IMAGE_DIR, file_name)\n",
    "    return np.array(Image.open(path), dtype=np.int)\n",
    "\n",
    "def process_image_in_parallel(image):\n",
    "    return (\n",
    "        convert_image_to_numpy_array(image[\"file_name\"]),\n",
    "        get_polygon_matrix(image[\"annotations\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44599db",
   "metadata": {},
   "source": [
    "The functions above take an `image` entry from the `jsonl` file and process the JSON contents. The `file_name` is used to load the image itself into memory before converting to a numpy array. Additionally, the `annotations` data is used to create a matrix of 0s and 1s indicating whether or not a specific pixel is part of a bounding box.\n",
    "\n",
    "Processing all images individually takes far too long, so the [`joblib`](https://joblib.readthedocs.io/en/latest/parallel.html) library is used to run the processing in parallel threads for each subset of the data. For the training set, this reduced the overall runtime of the processing from around 34 minutes to less than 500 seconds.\n",
    "\n",
    "Each processed image returns a tuple, so the [`joblib`](https://joblib.readthedocs.io/en/latest/parallel.html) library returns a list of tuples, where the first element is the input data (`X`) and the second element is the output data (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89175382",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "processed_data = Parallel(n_jobs=100)(delayed(process_image_in_parallel)(image) for image in tqdm(test_data))\n",
    "end = time.time()\n",
    "\n",
    "print(\"Processing data took\", round(end - start, 0), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afb6ac",
   "metadata": {},
   "source": [
    "Before splitting the data into training, testing, and validation sets, we must first convert the list of tuples into a list of lists. Here, the first list is all the input data points, while the second list is all the target data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = list(map(list, zip(*processed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50afb558",
   "metadata": {},
   "source": [
    "Finally, we should sense check the processing by inspecting a random sample of data points and ensuring both that the images look as we would expect, and that the bounding boxes are where they should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_display(X_subset, y_subset):\n",
    "    idx = random.sample(range(0, len(X_subset)), 1)[0]\n",
    "\n",
    "    cmap = matplotlib.colors.ListedColormap([np.array([0, 0, 0, 0]), 'lawngreen'])\n",
    "    bounds = [0., 0.5, 1.]\n",
    "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(X_subset[idx])\n",
    "    plt.imshow(y_subset[idx], interpolation='none', cmap=cmap, norm=norm, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_and_display(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5639c",
   "metadata": {},
   "source": [
    "# Part 2: the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79f934",
   "metadata": {},
   "source": [
    "At this point we have processed our data, but need an easy way to interface with the PyTorch models which will be defined later in this section. To achieve this, we implement a simple Dataset class. It would be possible to actually call the processing of each subset in the `__init__` method, but this isn't an important design decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.Tensor(self.X[idx])\n",
    "        y = torch.Tensor(self.y[idx].astype(int))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a2b0a",
   "metadata": {},
   "source": [
    "Now that the datasets are set up, we can pass them into a DataLoader to batch and shuffle them for input into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1c4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "test_dataset = CustomImageDataset(X=X, y=y)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_batch_count = len(test_dataloader.dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc39438",
   "metadata": {},
   "source": [
    "Next, we specify the device:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1855e2",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c6cc1",
   "metadata": {},
   "source": [
    "Now that we have created some dataloaders, we can define the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e48589",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPoolsAllowed(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(NoPoolsAllowed, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, (5,5), 3, (1,1))\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 12, (3,3), 2, (1,1))\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        \n",
    "        self.up1 = nn.Upsample((512,512))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(12, 1, (3,3), 1, (1,1))\n",
    "        self.up2 = nn.Upsample((2048,2048))\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = x.permute((0,3,1,2))\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99653228",
   "metadata": {},
   "source": [
    "We then define the second model, which takes inspiration from [UNet](https://arxiv.org/abs/1505.04597) (albeit simplified so as not to break MLTGPU, which it did anyway):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnspiredNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnspiredNet, self).__init__()\n",
    "        # Encoder Block 1\n",
    "        self.enc1_conv1 = nn.Conv2d(3, 8, kernel_size=3, padding=1)\n",
    "        self.enc1_bn1 = nn.BatchNorm2d(8)\n",
    "        self.enc1_conv2 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.enc1_bn2 = nn.BatchNorm2d(8)\n",
    "        self.enc1_relu = nn.ReLU()\n",
    "        self.enc1_pool = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # Encode Block 2\n",
    "        self.enc2_conv1 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.enc2_bn1 = nn.BatchNorm2d(16)\n",
    "        self.enc2_conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.enc2_bn2 = nn.BatchNorm2d(16)\n",
    "        self.enc2_relu = nn.ReLU()\n",
    "        self.enc2_pool = nn.MaxPool2d((2, 2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bot_conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bot_bn1 = nn.BatchNorm2d(32)\n",
    "        self.bot_conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bot_bn2 = nn.BatchNorm2d(32)\n",
    "        self.bot_relu = nn.ReLU()\n",
    "        \n",
    "        # Decoder Block 1\n",
    "        self.dec1_up = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2, padding=0)\n",
    "        self.dec1_conv1 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.dec1_bn1 = nn.BatchNorm2d(16)\n",
    "        self.dec1_conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.dec1_bn2 = nn.BatchNorm2d(16)\n",
    "        self.dec1_relu = nn.ReLU()\n",
    "        \n",
    "        # Decoder Block 2\n",
    "        self.dec2_up = nn.ConvTranspose2d(16, 8, kernel_size=2, stride=2, padding=0)\n",
    "        self.dec2_conv1 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.dec2_bn1 = nn.BatchNorm2d(8)\n",
    "        self.dec2_conv2 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.dec2_bn2 = nn.BatchNorm2d(8)\n",
    "        self.dec2_relu = nn.ReLU()\n",
    "        \n",
    "        self.f_conv = nn.Conv2d(8, 1, kernel_size=1, padding=0)\n",
    "        self.f_sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.permute((0,3,1,2))\n",
    "        \n",
    "        # Run through Encoder Block 1\n",
    "        enc1_x = self.enc1_relu(self.enc1_bn2(self.enc1_conv2(self.enc1_bn1(self.enc1_conv1(inputs)))))\n",
    "        enc1_p = self.enc1_pool(enc1_x)\n",
    "        \n",
    "        # Run through Encoder Block 2\n",
    "        enc2_x = self.enc2_relu(self.enc2_bn2(self.enc2_conv2(self.enc2_bn1(self.enc2_conv1(enc1_p)))))\n",
    "        enc2_p = self.enc2_pool(enc2_x)\n",
    "        \n",
    "        # Run through Bottleneck\n",
    "        bot_x = self.bot_relu(self.bot_conv2(self.bot_bn1(self.bot_conv1(enc2_p))))\n",
    "        \n",
    "        # Run through Decoder Block 1\n",
    "        dec1_x = self.dec1_relu(\n",
    "            self.dec1_conv2(\n",
    "                self.dec1_bn1(\n",
    "                    self.dec1_conv1(\n",
    "                        torch.cat( # The output of convTrans2D is concatenated with enc2_x -> output of last enc block\n",
    "                            [\n",
    "                                self.dec1_up(bot_x), # the first operation puts the bottleneck output through the convTrans2D\n",
    "                                enc2_x\n",
    "                            ],\n",
    "                            axis=1\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Run through Decoder 2\n",
    "        dec1_x = self.dec2_relu(\n",
    "            self.dec2_conv2(\n",
    "                self.dec2_bn1(\n",
    "                    self.dec2_conv1(\n",
    "                        torch.cat( # The output of convTrans2D is concatenated with enc2_x -> output of last enc block\n",
    "                            [\n",
    "                                self.dec2_up(dec1_x), # the first operation puts the bottleneck output through the convTrans2D\n",
    "                                enc1_x\n",
    "                            ],\n",
    "                            axis=1\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Run through Output\n",
    "        conv = self.f_conv(dec1_x)\n",
    "        return self.f_sig(conv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ba185f",
   "metadata": {},
   "source": [
    "Before training the model, let's define a helper function that will run the training and validation for us. The function takes a name, a model class, a device, an optimizer, a learning rate and the number of epochs and does all the training loop for us, before running through the validation data. For both datasets, the function stores the average loss per epoch and, along with the model parameters, saves these to a unique name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0819f1e",
   "metadata": {},
   "source": [
    "## Testing and Evaluation of the Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(raw_preds):\n",
    "    batch_predictions = []\n",
    "    for item in raw_preds:\n",
    "        batch_predictions.append(np.asarray(item.detach().cpu().numpy() > THRESHOLD, int))\n",
    "    return batch_predictions\n",
    "\n",
    "\n",
    "def get_evaluation_variables(parameter_file, device=\"cuda:2\"):\n",
    "    this_model = torch.load(parameter_file).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        this_model.eval()\n",
    "        \n",
    "        test_images = []\n",
    "        test_preds = []\n",
    "        test_truth = []\n",
    "        \n",
    "        #DEBUG\n",
    "        raw_preds = []\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "        test_loss = 0\n",
    "\n",
    "        for (x, y) in test_dataloader:\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "\n",
    "            batch_preds = this_model(x)\n",
    "            test_loss += loss_fn(batch_preds.squeeze(dim=1), y)\n",
    "            \n",
    "            test_images.append(x.detach().cpu())\n",
    "            \n",
    "            #DEBUG\n",
    "            raw_preds.append(batch_preds.squeeze(dim=1))\n",
    "            test_preds.extend(get_predictions(batch_preds.squeeze(dim=1)))\n",
    "            test_truth.append(y.detach().cpu())\n",
    "           \n",
    "    \n",
    "        avg_test_loss = test_loss / test_batch_count\n",
    "        test_preds = test_preds # this is the problematic one\n",
    "        test_images = test_images # np.concat\n",
    "        test_truth = test_truth # np.concat\n",
    "        \n",
    "        #DEBUG\n",
    "        raw_preds = raw_preds # np.concatenate\n",
    "    \n",
    "    return test_images, test_preds, test_truth, avg_test_loss, raw_preds\n",
    "\n",
    "\n",
    "def get_evaluation_metrics(preds, ground_truth, avg_test_loss, device=\"cpu\"):\n",
    "    prediction_tensor = torch.Tensor(preds).to(device)\n",
    "    ground_truth_tensor = torch.Tensor(ground_truth).to(device)\n",
    "    confusion_vector = prediction_tensor / ground_truth_tensor\n",
    "\n",
    "    tp = torch.sum(confusion_vector == 1).item()\n",
    "    fp = torch.sum(confusion_vector == float('inf')).item()\n",
    "    tn = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    fn = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    mse = mean_squared_error(\n",
    "        ground_truth.reshape(len(ground_truth_np)*2048*2048),\n",
    "        preds.reshape(len(preds)*2048*2048)\n",
    "    )\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "\n",
    "    try:\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 0\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Average Test Loss\": avg_test_loss.item(),\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Recall\": recall,\n",
    "        \"Precision\": precision,\n",
    "        \"F1-Score\": f1_score,\n",
    "        \"Mean Squared Error\": mse,\n",
    "    }, index=[\"Test Metrics\"]).round(4)\n",
    "\n",
    "\n",
    "def display_training_validation_loss(history_file):\n",
    "    history = pd.DataFrame(pickle.load(open(history_file, \"rb\")))\n",
    "    history[\"Epoch\"] = [e for e in range(1, len(history) + 1)]\n",
    "    history = history.rename(columns={\"train_loss\": \"Train Loss\", \"val_loss\": \"Validation Loss\"})\n",
    "\n",
    "    plt.plot(\"Epoch\", \"Train Loss\", data=history, marker='', color='skyblue', linewidth=2)\n",
    "    plt.plot(\"Epoch\", \"Validation Loss\", data=history, marker='', color='olive', linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def visualise_probability_map(images, preds, ground_truth, idx=None, save_name=None):\n",
    "    if not idx:\n",
    "        idx = random.sample(range(0, len(images)), 1)[0]\n",
    "    print(f\"Showing image #{idx}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))\n",
    "\n",
    "    cmap = matplotlib.colors.ListedColormap([[0, 0, 0, 0], 'lawngreen'])\n",
    "    bounds = np.array([0., 0.5, 1.],dtype=object,)\n",
    "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    ax1.imshow(images[idx].astype(int))\n",
    "    ax1.imshow(preds[idx], interpolation='none', cmap=cmap, alpha=1);\n",
    "    ax1.set_title(\"Predicted\")\n",
    "\n",
    "    ax2.imshow(images[idx].astype(int))\n",
    "    ax2.imshow(ground_truth[idx], interpolation='none', cmap=cmap, alpha=0.8)\n",
    "    ax2.set_title(\"Ground Truth\");\n",
    "    \n",
    "    if save_name:\n",
    "        fig.savefig(f\"{save_name}_{idx}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f09d73",
   "metadata": {},
   "source": [
    "## Evaluation for `NoPoolsAllowed`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087575fa",
   "metadata": {},
   "source": [
    "For the `NoPoolsAllowed` model, we use the `parameters_NoPoolsAllowed_Adam_0.01_20.pth` parameter file, as well as a `THRESHOLD` which sets the threshold for categorisation as a bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7912376",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.005\n",
    "\n",
    "images, preds, ground_truth, avg_test_loss, debug_preds = get_evaluation_variables(\n",
    "    parameter_file=\"./parameters_NoPoolsAllowed_Adam_0.01_20.pth\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9faef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_np = np.concatenate([img.detach().cpu().numpy() for img in images])\n",
    "ground_truth_np = np.concatenate([gt.detach().cpu().numpy() for gt in ground_truth])\n",
    "preds = np.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_metrics(preds, ground_truth_np, avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_validation_loss(history_file=\"history_NoPoolsAllowed_Adam_0.01_20.pic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139b9d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise_probability_map(images_np, preds, ground_truth_np, idx=2, save_name=\"NoPoolsAllowed_Adam_0.01_20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958c403",
   "metadata": {},
   "source": [
    "## Evaluation for `UnspiredNet`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63614dce",
   "metadata": {},
   "source": [
    "Since the `UnspiredNet` seemed to actually learn something, it was possible to use a much higher `THRESHOLD` for pixel categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.05\n",
    "\n",
    "images, preds, ground_truth, avg_test_loss, debug_preds = get_evaluation_variables(\n",
    "    parameter_file=\"./parameters_UnspiredNet_Adam_0.01_5.pth\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_np = np.concatenate([img.detach().cpu().numpy() for img in images])\n",
    "ground_truth_np = np.concatenate([gt.detach().cpu().numpy() for gt in ground_truth])\n",
    "preds = np.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405557c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_metrics(preds, ground_truth_np, avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_training_validation_loss(history_file=\"history_UnspiredNet_Adam_0.01_5.pic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fad9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualise_probability_map(images_np, preds, ground_truth_np, idx=2, save_name=\"UnspiredNet_Adam_0.01_5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
